{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Build binary classification models that predict activity/inactivity of small molecules against human aromatase using supervised learning methods.\n",
    "- Evaluate the performance of the developed models using performance measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import bioactivity data from PubChem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will develop a prediction model for small molecule's activity against human aromatase (https://pubchem.ncbi.nlm.nih.gov/protein/EAW77416), which is encoded by the CYP19A1 gene (https://pubchem.ncbi.nlm.nih.gov/gene/1588). The model will predict the activity of a molecule based on the structure of the molecule (represented with molecular fingerprints).\n",
    "\n",
    "For model development, we will use the Tox21 bioassay data for human aromatase, archived in PubChem (https://pubchem.ncbi.nlm.nih.gov/bioassay/743139).  The bioactivity data presented on this page can be downloaded by clicking the \"Download\" button available on this page and then read the data into a data frame.  Alternatively, you can directly load the data into a data frame as shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://pubchem.ncbi.nlm.nih.gov/assay/pcget.cgi?query=download&record_type=datatable&actvty=all&response_type=save&aid=743139'\n",
    "df_raw = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Lines 0-2 provide the descriptions for each column (data type, descriptions, units, etc).  These rows need be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw[3:]\n",
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names in this data frame contain white spaces and special characters.  For simplicity, let's rename the columns (no spaces or special characters except for the \"_\" character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_map = {'PUBCHEM_RESULT_TAG' : 'pc_result_tag', \n",
    "                 'PUBCHEM_SID' : 'sid', \n",
    "                 'PUBCHEM_CID' : 'cid',\n",
    "                 'PUBCHEM_ACTIVITY_OUTCOME' : 'activity_outcome', \n",
    "                 'PUBCHEM_ACTIVITY_SCORE' : 'activity_score',\n",
    "                 'PUBCHEM_ACTIVITY_URL' : 'activity_url', \n",
    "                 'PUBCHEM_ASSAYDATA_COMMENT' : 'assay_data_comment', \n",
    "                 'Activity Summary' : 'activity_summary',\n",
    "                 'Antagonist Activity' : 'antagonist_activity', \n",
    "                 'Antagonist Potency (uM)' : 'antagonist_potency', \n",
    "                 'Antagonist Efficacy (%)' : 'antagonist_efficacy',\n",
    "                 'Viability Activity' : 'viability_activity', \n",
    "                 'Viability Potency (uM)' : 'viability_potency',\n",
    "                 'Viability Efficacy (%)' : 'viability_efficacy', \n",
    "                 'Sample Source' : 'sample_source' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw.rename(columns = col_names_map)\n",
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check the number of compounds for each activity group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to understand what our data look like.  Especially, we are interested in the activity class of the tested compounds because we are developing a model that classifies small molecules according to their activities against the target.  This information is available in the \"**activity_outcome**\" and \"**activity_summary**\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.groupby(['activity_outcome']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data in the **activity_outcome** column, there are 379 actives, 7562 inactives, and 2545 inconclusives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.groupby(['activity_outcome','activity_summary']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that, in the **activity_summary** column, the inconclusive compounds are further classified into subclasses, which include:\n",
    "\n",
    "- **active agonist**\n",
    "- inconclusive\n",
    "- inconclusive agonist\n",
    "- inconclusive antagonist\n",
    "- inconclusive agonist (cytotoxic)\n",
    "- inconclusive antagonist (cytotoxic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As implied in the title of this assay record (https://pubchem.ncbi.nlm.nih.gov/bioassay/743139), this assay aims to identify **aromatase inhibitors**.  Therefore, all **active antagonists** (in the activity summary column) were declared to be **active** compounds (in the activity outcome column).\n",
    "\n",
    "On the other hand, the assay also identified 612 **active agonists** (in the activity summary column), and they are declared to be **inconclusive** (in the activity outcome column).\n",
    "\n",
    "With that said, \"inactive\" compounds in this assay means those which are neither active agonists nor active antagonist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that the criteria used for determining whether a compound is active or not in a given assay are selected by the data source who submitted that assay data to PubChem.  For the purpose of this assignment (which aims to develop a binary classifier that tells if a molecule is active or inactive against the target), we should clarify what we mean by \"active\" and \"inactive\".\n",
    "\n",
    "- **active** : any compounds that can change (either increase or decrease) the activity of the target.  This is equivalent to either **active antagonists** or **active agonists** in the activity summary column.\n",
    "- **inactive** : any compounds that do not change the activity of the target.  This is equivalent to **inactive** compounds in the activity summary column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select active/inactive compounds for model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to select only the active and inactive compounds from the data frame (that is, active agonists, active antagonists, and inactives based on the \"activity summary\" column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw[ (df_raw['activity_summary'] == 'active agonist' ) | \n",
    "             (df_raw['activity_summary'] == 'active antagonist' ) |\n",
    "             (df_raw['activity_summary'] == 'inactive' ) ]\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['sid'].unique()))\n",
    "print(len(df['cid'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of CIDs is not the same as the number of SIDs.  There are two important potential reasons for this observation.  \n",
    "\n",
    "First, not all substances (SIDs) in PubChem have associated compounds (CIDs) because some substances failed during structure standardization.  \\[Remember that, in PubChem, substances are depositor-provided structures and compounds are unique structures extracted from substances through structure standardization.]  Because our model will use structural information of molecules to predict their bioactivity, we need to remove substances without associated CIDs (i.e., no standardized structures).\n",
    "\n",
    "Second, some compounds are associated with more than one substances.  In the context of this assay, it means that a compound may be tested multiple times in different samples (which are designated as different substances).  It is not uncommon that different samples of the same chemical may result in conflicting activities (e.g., active agonist in one sample but inactive in another sample).  In this practice, we remove such compounds with conflicting activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-(1) Drop substances without associated CIDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check if there are subtances without associated CIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 138 records whose \"cid\" column is NULL, and we want to remove those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna( subset=['cid'] )\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['sid'].unique()))\n",
    "print(len(df['cid'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()   # Check if the NULL values disappeared in the \"cid\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-(2) Remove CIDs with conflicting activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now identify compounds with conflicting activities and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_conflict = []\n",
    "idx_conflict = []\n",
    "\n",
    "for mycid in df['cid'].unique() :\n",
    "    \n",
    "    outcomes = df[ df.cid == mycid ].activity_summary.unique()\n",
    "    \n",
    "    if len(outcomes) > 1 :\n",
    "        \n",
    "        idx_tmp = df.index[ df.cid == mycid ].tolist()\n",
    "        idx_conflict.extend(idx_tmp)\n",
    "        cid_conflict.append(mycid)\n",
    "\n",
    "print(\"#\", len(cid_conflict), \"CIDs with conflicting activities [associated with\", len(idx_conflict), \"rows (SIDs).]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_conflict,:].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(idx_conflict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('activity_summary').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['sid'].unique()))\n",
    "print(len(df['cid'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-(3) Remove redundant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code cells \\[in 3-(2)] do not remove compounds tested multiple times if the testing results are consistent [e.g., active agonist in all samples (substances)].  The rows corresponding to these compounds are redundant, so we want remove them except for only one row for each compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='cid')  # remove duplicate rows except for the first occurring row.\n",
    "print(len(df['sid'].unique()))\n",
    "print(len(df['cid'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-(4) Adding \"numeric\" activity classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the inputs and outputs to machine learning algorithms need to have numerical forms.   \n",
    "In this practice, the input (molecular structure) will be represented with binary fingerprints, which already have numerical forms (0 or 1).  However, the output (activity) is currently in a string format (e.g., 'active agonist', 'active antagonist').  Therefore, we want to add an additional, 'activity' column, which contains numeric codes representing the active and inactive compounds:\n",
    "- 1 for actives (either active agonists or active antagonists)\n",
    "- 0 for inactives\n",
    "\n",
    "Note that we are merging the two classes \"active agonist\" and \"active antagonist\", because we are going to build a binary classifer that distinguish actives from inactives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['activity'] = [ 0 if x == 'inactive' else 1 for x in df['activity_summary'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the new column 'activity' is added to (the end of) the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-check the count of active/inactive compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('activity_summary').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('activity').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-(5) Create a smaller data frame that only contains CIDs and activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a smaller data frame that only contains CIDs and activities.  This data frame will be merged with a data frame containing molecular fingerprint information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity = df[['cid','activity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download structure information for each compound from PubChem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get structure information of the compounds from PubChem (in isomeric SMILES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cids = df.cid.astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 200\n",
    "num_cids = len(cids)\n",
    "\n",
    "if num_cids % chunk_size == 0 :\n",
    "    num_chunks = int( num_cids / chunk_size )\n",
    "else :\n",
    "    num_chunks = int( num_cids / chunk_size ) + 1\n",
    "\n",
    "print(\"# CIDs = \", num_cids)\n",
    "print(\"# CID Chunks = \", num_chunks, \"(chunked by \", chunk_size, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "df_smiles = pd.DataFrame()\n",
    "list_dfs = []  # temporary list of data frames\n",
    "\n",
    "for i in range(0, num_chunks) :\n",
    "    \n",
    "    idx1 = chunk_size * i\n",
    "    idx2 = chunk_size * (i + 1)\n",
    "    cidstr = \",\".join( str(x) for x in cids[idx1:idx2] )\n",
    "\n",
    "    url = ('https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/' + cidstr + '/property/IsomericSMILES/TXT')\n",
    "    res = requests.get(url)\n",
    "    data = pd.read_csv( StringIO(res.text), header=None, names=['smiles'] )\n",
    "    list_dfs.append(data)\n",
    "    \n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    if ( i % 5 == 0 ) :\n",
    "        print(\"Processing Chunk \", i)   \n",
    "\n",
    "#    if ( i == 2 ) : break  #- for debugging\n",
    "\n",
    "df_smiles = pd.concat(list_dfs,ignore_index=True)\n",
    "df_smiles[ 'cid' ] = cids\n",
    "df_smiles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smiles = df_smiles[['cid','smiles']]\n",
    "df_smiles.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate MACCS keys from SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps=dict()\n",
    "\n",
    "for idx, row in df_smiles.iterrows() :\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(row.smiles)\n",
    "    \n",
    "    if mol == None :\n",
    "        print(\"Can't generate MOL object:\", \"CID\", row.cid, row.smiles)\n",
    "    else:\n",
    "        fps[row.cid] = [row.cid] + list(MACCSkeys.GenMACCSKeys(mol).ToBitString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate column names\n",
    "fpbitnames = []\n",
    "\n",
    "fpbitnames.append('cid')\n",
    "\n",
    "for i in range(0,167):   # from MACCS000 to MACCS166\n",
    "    fpbitnames.append( \"maccs\" + str(i).zfill(3) )\n",
    "\n",
    "df_fps = pd.DataFrame.from_dict(fps, orient='index', columns=fpbitnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fps.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Merge activity data and fingerprint information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fps.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_activity.join(df_fps.set_index('cid'), on='cid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section 5, there were two CIDs for which the MACCS keys could not be generated.  They need to be removed from **df_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[df_data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.dropna()\n",
    "len(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save df_data in CSV for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv('df_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preparation for model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-(1) Loading the data into X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_data.iloc[:,2:]\n",
    "y = df_data['activity'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y))    # Number of all compounds\n",
    "y.sum()          # Number of actives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-(2) Remove zero-variance features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features in X are not helpful in distinguishing actives from inactives, because they are set ON for all compounds or OFF for all compounds.  Such features need to be removed because they would consume more computational resources without improving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape  #- Before removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = VarianceThreshold()\n",
    "X=sel.fit_transform(X)\n",
    "X.shape  #- After removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, four features had zero variances.  Note that one of them is the first bit (maccs000) of the MACCS keys, which is added as a \"dummy\" to name each of bits 1~166 as maccs001, maccs002, ... maccs166.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-(3) Train-Test-Split (a 9:1 ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the data set into a training set (90%) and test set (10%).  The training set will be used to train the model.  The developed model will be tested against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, shuffle=True, random_state=3100, stratify=y, test_size=0.1)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(y_train.sum(), y_test.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-(4) Balance the training set through downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dimension of the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_train))\n",
    "print(len(X_train))\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of actives and inactives compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# inactives : \", len(y_train) - y_train.sum())\n",
    "print(\"# actives   : \", y_train.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is highly imbalanced \\[the inactive to active ratio is 8.16 (=5448 / 668)].  To address this issue, let's downsample the majority class (inactive compounds) to balance the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicies of each class' observations\n",
    "idx_inactives = np.where( y_train == 0 )[0]\n",
    "idx_actives   = np.where( y_train == 1 )[0]\n",
    "\n",
    "# Number of observations in each class\n",
    "num_inactives = len(idx_inactives)\n",
    "num_actives   = len(idx_actives)\n",
    "\n",
    "# Randomly sample from inactives without replacement\n",
    "np.random.seed(0)\n",
    "idx_inactives_downsampled = np.random.choice(idx_inactives, size=num_actives, replace=False)\n",
    "\n",
    "# Join together downsampled inactives with actives\n",
    "X_train = np.vstack((X_train[idx_inactives_downsampled], X_train[idx_actives]))\n",
    "y_train = np.hstack((y_train[idx_inactives_downsampled], y_train[idx_actives]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noteworthy that **np.vstack** is used for X_train and **np.hstack** is used for Y_train.  The direction of stacking is different because X_train is a 2-D array and y_train is a 1-D array.\n",
    "\n",
    "Confirm that the downsampled data set has the correct dimension and active/inactive counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# inactives : \", len(y_train) - y_train.sum())\n",
    "print(\"# actives   : \", y_train.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_train))\n",
    "print(len(X_train))\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Build a model using the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build predictive models using machine learning algorithms available in the scikit-learn library (https://scikit-learn.org/).  This notebook will use Naive Bayes and decisiont tree, because they are relatively fast and simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB        #-- Naive Bayes\n",
    "from sklearn.tree import DecisionTreeClassifier    #-- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-(1) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BernoulliNB()            # set up the NB classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit( X_train ,y_train )    # Train the model by fitting it to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_train, clf.predict( X_train )    # Apply the model to predict the training compound's activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMat = confusion_matrix( y_true, y_pred )    #-- generate confusion matrix\n",
    "print(CMat)    # [[TN, FP], \n",
    "               #  [FN, TP]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc  = accuracy_score( y_true, y_pred )\n",
    "\n",
    "sens = CMat[ 1 ][ 1 ] / ( CMat[ 1 ][ 0 ] + CMat[ 1 ][ 1 ] )    # TP / (FN + TP)\n",
    "spec = CMat[ 0 ][ 0 ] / ( CMat[ 0 ][ 0 ] + CMat[ 0 ][ 1 ] )    # TN / (TN + FP )\n",
    "bacc = (sens + spec) / 2\n",
    "\n",
    "y_score = clf.predict_proba( X_train )[:, 1]\n",
    "auc = roc_auc_score( y_true, y_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#-- Accuracy          = \", acc)\n",
    "print(\"#-- Balanced Accuracy = \", bacc)\n",
    "print(\"#-- Sensitivity       = \", sens)\n",
    "print(\"#-- Specificity       = \", spec)\n",
    "print(\"#-- AUC-ROC           = \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applied to predict the activity of the training compounds, the NB classifier resulted in the accuracy of 0.70 and AUC-ROC of 0.75.  However, the real performance of the model should be evaluated with the test set data, which are not used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_test, clf.predict(X_test)    #-- Apply the model to predict the test set compounds' activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMat = confusion_matrix( y_true, y_pred )    #-- generate confusion matrix\n",
    "print(CMat)    # [[TN, FP], \n",
    "               #  [FN, TP]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc  = accuracy_score( y_true, y_pred )\n",
    "\n",
    "sens = CMat[ 1 ][ 1 ] / ( CMat[ 1 ][ 0 ] + CMat[ 1 ][ 1 ] )\n",
    "spec = CMat[ 0 ][ 0 ] / ( CMat[ 0 ][ 0 ] + CMat[ 0 ][ 1 ] )\n",
    "bacc = (sens + spec) / 2\n",
    "\n",
    "y_score = clf.predict_proba( X_test )[:, 1]\n",
    "auc = roc_auc_score( y_true, y_score )\n",
    "\n",
    "print(\"#-- Accuracy          = \", acc)\n",
    "print(\"#-- Balanced Accuracy = \", bacc)\n",
    "print(\"#-- Sensitivity       = \", sens)\n",
    "print(\"#-- Specificity       = \", spec)\n",
    "print(\"#-- AUC-ROC           = \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set, the accuracy is 0.67 and the AUC-ROC is 0.72.  These values are somewhat smaller (by 0.03) than those for the training set.  Also note that the accuracy is no longer the same as the balanced accruacy (which is the average of the sensitivity and specificity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional performance information may be obtained using **classification_report()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-(2) Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier( random_state=0 )    # set up the DT classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit( X_train ,y_train )    # Train the model by fitting it to the data (using the default values for all parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_train, clf.predict( X_train )    # Apply the model to predict the training compound's activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMat = confusion_matrix( y_true, y_pred )    #-- generate confusion matrix\n",
    "print(CMat)    # [[TN, FP], \n",
    "               #  [FN, TP]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc  = accuracy_score( y_true, y_pred )\n",
    "\n",
    "sens = CMat[ 1 ][ 1 ] / ( CMat[ 1 ][ 0 ] + CMat[ 1 ][ 1 ] )    # TP / (FN + TP)\n",
    "spec = CMat[ 0 ][ 0 ] / ( CMat[ 0 ][ 0 ] + CMat[ 0 ][ 1 ] )    # TN / (TN + FP )\n",
    "bacc = (sens + spec) / 2\n",
    "\n",
    "y_score = clf.predict_proba( X_train )[:, 1]\n",
    "auc = roc_auc_score( y_true, y_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#-- Accuracy          = \", acc)\n",
    "print(\"#-- Balanced Accuracy = \", bacc)\n",
    "print(\"#-- Sensitivity       = \", sens)\n",
    "print(\"#-- Specificity       = \", spec)\n",
    "print(\"#-- AUC-ROC           = \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applied to predict the activity of the **training** compounds, the DT classifier resulted in very high scores (>0.99) for all five performance measures considered here.  However, it does **not** necessarily mean that the model will perform very well for the **test** set compounds.  Let's apply the model to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_test, clf.predict(X_test)    #-- Apply the model to predict the test set compounds' activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMat = confusion_matrix( y_true, y_pred )    #-- generate confusion matrix\n",
    "print(CMat)    # [[TN, FP], \n",
    "               #  [FN, TP]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc  = accuracy_score( y_true, y_pred )\n",
    "\n",
    "sens = CMat[ 1 ][ 1 ] / ( CMat[ 1 ][ 0 ] + CMat[ 1 ][ 1 ] )\n",
    "spec = CMat[ 0 ][ 0 ] / ( CMat[ 0 ][ 0 ] + CMat[ 0 ][ 1 ] )\n",
    "bacc = (sens + spec) / 2\n",
    "\n",
    "y_score = clf.predict_proba( X_test )[:, 1]\n",
    "auc = roc_auc_score( y_true, y_score )\n",
    "\n",
    "print(\"#-- Accuracy          = \", acc)\n",
    "print(\"#-- Balanced Accuracy = \", bacc)\n",
    "print(\"#-- Sensitivity       = \", sens)\n",
    "print(\"#-- Specificity       = \", spec)\n",
    "print(\"#-- AUC-ROC           = \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the DT model was applied to the test set, all performance measures were much worse than those for the training set.  This is a typical example of **outfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model building through cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above section, the models were developed using the default values for many optional hyperparamters, which cannot be learned by the training algorithm.  For example, when building a decision tree model, one should specify how the tree should be deep, how many compounds should be allowed in a single leaf, what is the minimum number of compounds in a single leaf, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below demonstrate how to perform hyperparameter optimization through 10-fold cross-validation.  In this example, five values for each of three hyperparameters used in decision tree are considered (max_depth, min_samples_split, and min_samples_leaf), resulting in a total of 125 combination of the parameter values (=5 x 5 x 5).  For each combination, 10 models are generated (through 10-fold cross validation) and the average performance will be tracked.  The goal is to find the parameter value combination that results in the highest average performance score (e.g., 'roc_auc') from the 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [ 'roc_auc', 'balanced_accuracy' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncvs = 10\n",
    "\n",
    "max_depth_range         = np.linspace( 3, 7, num=5, dtype='int32' )\n",
    "min_samples_split_range = np.linspace( 3, 7, num=5, dtype='int32' )\n",
    "min_samples_leaf_range  = np.linspace( 2, 6, num=5, dtype='int32' )\n",
    "\n",
    "param_grid = dict( max_depth=max_depth_range,\n",
    "                   min_samples_split=min_samples_split_range,\n",
    "                   min_samples_leaf=min_samples_leaf_range )\n",
    "\n",
    "clf = GridSearchCV( DecisionTreeClassifier( random_state=0 ),\n",
    "                    param_grid=param_grid, cv=ncvs, scoring=scores, refit='roc_auc',\n",
    "                    return_train_score = True, iid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit( X_train, y_train )\n",
    "print(\"Best parameter set\", clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, it is possible to look into the performance data for each parameter value combination (stored in **clf.cv_results_**), as shown in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_1a = clf.cv_results_['mean_train_roc_auc']\n",
    "stds_1a  = clf.cv_results_['std_train_roc_auc']\n",
    "\n",
    "means_1b = clf.cv_results_['mean_test_roc_auc']\n",
    "stds_1b  = clf.cv_results_['std_test_roc_auc']\n",
    "\n",
    "means_2a = clf.cv_results_['mean_train_balanced_accuracy']\n",
    "stds_2a  = clf.cv_results_['std_train_balanced_accuracy']\n",
    "\n",
    "means_2b = clf.cv_results_['mean_test_balanced_accuracy']\n",
    "stds_2b  = clf.cv_results_['std_test_balanced_accuracy']\n",
    "\n",
    "iterobjs = zip( means_1a, stds_1a, means_1b, stds_1b,\n",
    "                means_2a, stds_2a, means_2b, stds_2b, clf.cv_results_['params'] )\n",
    "\n",
    "for m1a, s1a, m1b, s1b, m2a, s2a, m2b, s2b, params in iterobjs :\n",
    "\n",
    "    print( \"Grid %r : %0.4f %0.04f %0.4f %0.04f %0.4f %0.04f %0.4f %0.04f\"\n",
    "           % ( params, m1a, s1a, m1b, s1b, m2a, s2a, m2b, s2b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell to look into additional performance data stored in cv_result_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clf.cv_result_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that each model built through 10-fold cross-validation during hyperparameter optimization uses only 90% of the compounds in the training set and the remaining 10% is used for testing that model.  After all parameter value combinations are evaluated, the best parameter values are selected and used to rebuild a model from **all** compounds in the training set.  **GridSearchCV()** takes care of this last step automatically.  Therefore, there is no need to take an extra step to build a model using **cls.fit()** after hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_train, clf.predict( X_train )    # Apply the model to predict the training compound's activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMat = confusion_matrix( y_true, y_pred )    #-- generate confusion matrix\n",
    "print(CMat)    # [[TN, FP], \n",
    "               #  [FN, TP]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc  = accuracy_score( y_true, y_pred )\n",
    "\n",
    "sens = CMat[ 1 ][ 1 ] / ( CMat[ 1 ][ 0 ] + CMat[ 1 ][ 1 ] )    # TP / (FN + TP)\n",
    "spec = CMat[ 0 ][ 0 ] / ( CMat[ 0 ][ 0 ] + CMat[ 0 ][ 1 ] )    # TN / (TN + FP )\n",
    "bacc = (sens + spec) / 2\n",
    "\n",
    "y_score = clf.predict_proba( X_train )[:, 1]\n",
    "auc = roc_auc_score( y_true, y_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#-- Accuracy          = \", acc)\n",
    "print(\"#-- Balanced Accuracy = \", bacc)\n",
    "print(\"#-- Sensitivity       = \", sens)\n",
    "print(\"#-- Specificity       = \", spec)\n",
    "print(\"#-- AUC-ROC           = \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these performance data with those from section 8-(2) (for the training set).  When the default values were used, the DT model gave >0.99 for all performance measures, but the current models (developed using hyperparameter optimization) have much lower values, ranging from 0.73 to 0.83.  Again, however, what really matters is the performance against the test set, which contains the data not used for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_test, clf.predict(X_test)    #-- Apply the model to predict the test set compounds' activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMat = confusion_matrix( y_true, y_pred )    #-- generate confusion matrix\n",
    "print(CMat)    # [[TN, FP], \n",
    "               #  [FN, TP]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc  = accuracy_score( y_true, y_pred )\n",
    "\n",
    "sens = CMat[ 1 ][ 1 ] / ( CMat[ 1 ][ 0 ] + CMat[ 1 ][ 1 ] )\n",
    "spec = CMat[ 0 ][ 0 ] / ( CMat[ 0 ][ 0 ] + CMat[ 0 ][ 1 ] )\n",
    "bacc = (sens + spec) / 2\n",
    "\n",
    "y_score = clf.predict_proba( X_test )[:, 1]\n",
    "auc = roc_auc_score( y_true, y_score )\n",
    "\n",
    "print(\"#-- Accuracy          = \", acc)\n",
    "print(\"#-- Balanced Accuracy = \", bacc)\n",
    "print(\"#-- Sensitivity       = \", sens)\n",
    "print(\"#-- Specificity       = \", spec)\n",
    "print(\"#-- AUC-ROC           = \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the model from hyperparameter optimization gives better performance data against the test set, compared to the model developed using the default parameter values.  Importantly, the model from hyperparameter optimization shows smaller differences in performance measures between the training and test sets, indicatiing that the issue of outffiting has been alleviated substantially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will build predictive models using the same aromatase data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 1** Show the following information to make sure that the activity data in the **df_activity** data frame is still available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first five lines of **df_activity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The counts of active/inactive compounds in **df_activity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2** Show the following information to make sure the structure data is still available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first five lines of **df_smiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the number of rows of **df_smiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3** Generate the (ECFP-equivalent) circular fingerprints from the SMILES strings.\n",
    "- Use RDKit to generate 1024-bit-long circular fingerprints.\n",
    "- Set the radius of the circular fingerprint to 2.\n",
    "- Store the fingerprints in a data_frame called **df_fps** (along with the CIDs).\n",
    "- Print the dimension of **df_fps**.\n",
    "- Show the first five lines of **df_fps**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4** Merge the **df_activity** and **df_fps** data frames into a data frame called **df_data**\n",
    "- Join the two data frames using the CID column as keys.\n",
    "- Remove the rows that have any NULL values (i.e., compounds for which the fingerprints couldn't be generated).\n",
    "- Print the dimension of **df_data**.\n",
    "- Show the first five lines of **df_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5** Prepare input and output data for model building\n",
    "- Load the fingerprint data into 2-D array (X) and the activity data into 1-D array (y).\n",
    "- Show the dimension of X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove zero-variance features from X (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the data set into training and test sets (90% vs 10%) (using random_state=3100).\n",
    "- Print the dimension of X and y for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Balance the training data set through downsampling.\n",
    "- Show the number of inactive/active compounds in the downsampled training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6** Building a Random Forest model using the balanced training data set.\n",
    "- First read the followng documents about random forest (https://scikit-learn.org/stable/modules/ensemble.html#forest and https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier). \n",
    "- Use 10-fold cross validation to select the best value for the \"n_estimators\" parameter that maximizes the **balanced accuracy**.  Test 40 values from 5 to 200 with an increment of 5 (e.g., 5, 10, 15, 20, ..., 190, 195, 200).\n",
    "- For parameters 'max_depth', 'min_samples_leaf', and 'min_samples_split', use the best values found in Section 9.\n",
    "- For other parameters, use the default values.\n",
    "- For each parameter value, print the mean balanced accuracies (for both training and test from cross validation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7** Apply the developed RF model to predict the activity of the **training** set compounds.\n",
    "\n",
    "- Report the confusion matrix.\n",
    "- Report the accuracy, balanced accurayc, sensitivity, specificity, and auc-roc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8** Apply the developed RF model to predict the activity of the **test** set compounds.\n",
    "\n",
    "- Report the accuracy, balanced accurayc, sensitivity, specificity, and auc-roc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9** Read a recent paper published in *Chem. Res. Toxicol.* (https://doi.org/10.1021/acs.chemrestox.7b00037) and answer the following questions (in no more than five sentences for each question).\n",
    "\n",
    "- What different approaches did the paper take to develop prediction models (compared to those used in this notebook)?\n",
    "- How different are the models reported in the paper from those constructed in this paper (in terms of the performance measures)? \n",
    "- What would you do to develop models with improved performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answers in this cell.\n",
    "\n",
    "- \n",
    "\n",
    "- \n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (MolSSI)",
   "language": "python",
   "name": "molssi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
